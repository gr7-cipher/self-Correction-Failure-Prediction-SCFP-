{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCFP Framework - Interactive Demo\n",
    "\n",
    "This notebook provides an interactive demonstration of the Self-Correction Failure Prediction (SCFP) framework.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The SCFP framework predicts when LLM self-correction will fail and routes correction strategies accordingly. This demo shows:\n",
    "\n",
    "1. **Synthetic Data Generation**: Create realistic correction traces\n",
    "2. **Failure Prediction**: Use DeBERTa-v3 to predict correction outcomes\n",
    "3. **Dynamic Routing**: Intelligently select correction strategies\n",
    "4. **Cost-Benefit Analysis**: Optimize accuracy vs cost trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# SCFP imports\n",
    "from scfp.data.dataset import CorrectionTrace, FailureMode, SCFPDataset\n",
    "from scfp.data.synthetic import SyntheticDataGenerator, SyntheticConfig\n",
    "from scfp.routing.router import DynamicRouter, RoutingStrategy\n",
    "from scfp.routing.cost_model import CostModel\n",
    "from scfp.models.baselines import BaselineModels\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"SCFP Framework Interactive Demo\")\n",
    "print(\"===============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "First, let's generate some synthetic correction traces to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure synthetic data generation\n",
    "config = SyntheticConfig(\n",
    "    total_samples=100,\n",
    "    success_rate=0.6,\n",
    "    failure_distribution={\n",
    "        \"jh\": 0.25,  # Justification Hallucination\n",
    "        \"cm\": 0.20,  # Confidence Miscalibration\n",
    "        \"ba\": 0.20,  # Bias Amplification\n",
    "        \"oc\": 0.20,  # Over-correction\n",
    "        \"rm\": 0.15   # Reasoning Myopia\n",
    "    },\n",
    "    domains=[\"math\", \"science\", \"history\", \"logic\"]\n",
    ")\n",
    "\n",
    "# Generate traces\n",
    "generator = SyntheticDataGenerator(config=config, seed=42)\n",
    "traces = generator.generate_traces()\n",
    "\n",
    "print(f\"Generated {len(traces)} correction traces\")\n",
    "\n",
    "# Show distribution\n",
    "success_count = sum(1 for trace in traces if trace.is_success)\n",
    "failure_count = len(traces) - success_count\n",
    "\n",
    "print(f\"Success: {success_count} ({success_count/len(traces)*100:.1f}%)\")\n",
    "print(f\"Failure: {failure_count} ({failure_count/len(traces)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize failure mode distribution\n",
    "mode_counts = {}\n",
    "for trace in traces:\n",
    "    mode = trace.failure_mode.value\n",
    "    mode_counts[mode] = mode_counts.get(mode, 0) + 1\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(mode_counts.values(), labels=mode_counts.keys(), autopct='%1.1f%%')\n",
    "plt.title('Failure Mode Distribution')\n",
    "\n",
    "# Domain distribution\n",
    "domain_counts = {}\n",
    "for trace in traces:\n",
    "    domain = trace.metadata.get(\"domain\", \"unknown\") if trace.metadata else \"unknown\"\n",
    "    domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(domain_counts.values(), labels=domain_counts.keys(), autopct='%1.1f%%')\n",
    "plt.title('Domain Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine Sample Traces\n",
    "\n",
    "Let's look at some example correction traces to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample traces\n",
    "def display_trace(trace, index):\n",
    "    \"\"\"Display a correction trace in a nice format.\"\"\"\n",
    "    status = \"‚úÖ SUCCESS\" if trace.is_success else \"‚ùå FAILURE\"\n",
    "    mode = trace.failure_mode.value.upper()\n",
    "    domain = trace.metadata.get(\"domain\", \"unknown\") if trace.metadata else \"unknown\"\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"border: 1px solid #ddd; padding: 15px; margin: 10px 0; border-radius: 5px;\">\n",
    "        <h4>Trace {index + 1} - {status} ({mode}) - Domain: {domain.title()}</h4>\n",
    "        <p><strong>Prompt:</strong> {trace.prompt}</p>\n",
    "        <p><strong>Initial Response:</strong> {trace.initial_response}</p>\n",
    "        <p><strong>Critique:</strong> {trace.critique}</p>\n",
    "        <p><strong>Final Response:</strong> {trace.final_response}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Show first 5 traces\n",
    "print(\"Sample Correction Traces:\")\n",
    "print(\"========================\")\n",
    "for i in range(min(5, len(traces))):\n",
    "    display_trace(traces[i], i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model Evaluation\n",
    "\n",
    "Let's evaluate some baseline approaches on our synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for baseline evaluation\n",
    "trace_texts = []\n",
    "binary_labels = []\n",
    "multiclass_labels = []\n",
    "\n",
    "for trace in traces:\n",
    "    text = f\"Prompt: {trace.prompt} [SEP] Initial Response: {trace.initial_response} [SEP] Critique: {trace.critique}\"\n",
    "    trace_texts.append(text)\n",
    "    binary_labels.append(1 if trace.is_success else 0)\n",
    "    \n",
    "    # Map failure mode to index\n",
    "    mode_to_idx = {\n",
    "        \"success\": 0, \"jh\": 1, \"cm\": 2, \"ba\": 3, \"oc\": 4, \"rm\": 5\n",
    "    }\n",
    "    multiclass_labels.append(mode_to_idx[trace.failure_mode.value])\n",
    "\n",
    "print(f\"Prepared {len(trace_texts)} traces for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline models\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "baselines = {\n",
    "    \"Random\": BaselineModels.get_random_baseline(42),\n",
    "    \"Confidence\": BaselineModels.get_confidence_heuristic(),\n",
    "    \"Length\": BaselineModels.get_length_heuristic(),\n",
    "    \"GPT-4o (Sim)\": BaselineModels.get_gpt4o_judge(42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, baseline in baselines.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    # Get predictions\n",
    "    binary_probs = baseline.predict_failure_probability(trace_texts)\n",
    "    binary_preds = (binary_probs < 0.5).astype(int)  # Failure prob < 0.5 means success\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(binary_labels, binary_preds)\n",
    "    f1 = f1_score(binary_labels, binary_preds, average='macro')\n",
    "    \n",
    "    # For AUC, we need success probabilities\n",
    "    success_probs = 1 - binary_probs\n",
    "    auc = roc_auc_score(binary_labels, success_probs)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Macro F1\": f1,\n",
    "        \"AUC-ROC\": auc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\")\n",
    "\n",
    "print(\"\\nBaseline evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize baseline results\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "metrics = [\"Accuracy\", \"Macro F1\", \"AUC-ROC\"]\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    bars = ax.bar(results_df.index, results_df[metric], alpha=0.7)\n",
    "    ax.set_title(f'{metric}')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, results_df[metric]):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display results table\n",
    "display(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic Routing System\n",
    "\n",
    "Now let's demonstrate the dynamic routing system that intelligently selects correction strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize routing system\n",
    "cost_model = CostModel()\n",
    "router = DynamicRouter(\n",
    "    failure_predictor=None,  # Using simulated predictions\n",
    "    cost_model=cost_model\n",
    ")\n",
    "\n",
    "print(\"Dynamic Router initialized\")\n",
    "print(\"Available strategies:\", [s.value for s in RoutingStrategy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate routing on sample traces\n",
    "sample_traces = traces[:10]  # Use first 10 traces\n",
    "routing_decisions = []\n",
    "\n",
    "for i, trace in enumerate(sample_traces):\n",
    "    # Create context based on trace metadata\n",
    "    domain = trace.metadata.get(\"domain\", \"general\") if trace.metadata else \"general\"\n",
    "    \n",
    "    context = {\n",
    "        \"domain\": domain,\n",
    "        \"urgency\": np.random.uniform(0.1, 0.9),\n",
    "        \"stakes\": np.random.uniform(0.2, 0.8),\n",
    "        \"accuracy_requirement\": np.random.uniform(0.5, 0.9)\n",
    "    }\n",
    "    \n",
    "    # Make routing decision\n",
    "    decision = router.route(\n",
    "        prompt=trace.prompt,\n",
    "        initial_response=trace.initial_response,\n",
    "        critique=trace.critique,\n",
    "        context=context\n",
    "    )\n",
    "    \n",
    "    routing_decisions.append(decision)\n",
    "    \n",
    "    print(f\"\\nTrace {i+1} ({domain})\")\n",
    "    print(f\"  Strategy: {decision.strategy.value.upper()}\")\n",
    "    print(f\"  Failure Prob: {decision.failure_probability:.3f}\")\n",
    "    print(f\"  Expected Accuracy: {decision.expected_accuracy:.3f}\")\n",
    "    print(f\"  Cost: {decision.cost_estimate:.3f}\")\n",
    "    print(f\"  Reasoning: {decision.reasoning[:100]}...\")\n",
    "\n",
    "print(f\"\\nProcessed {len(routing_decisions)} routing decisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze routing patterns\n",
    "strategy_counts = {}\n",
    "for decision in routing_decisions:\n",
    "    strategy = decision.strategy.value\n",
    "    strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Strategy distribution\n",
    "axes[0, 0].pie(strategy_counts.values(), labels=strategy_counts.keys(), autopct='%1.1f%%')\n",
    "axes[0, 0].set_title('Strategy Distribution')\n",
    "\n",
    "# Failure probability vs expected accuracy\n",
    "failure_probs = [d.failure_probability for d in routing_decisions]\n",
    "expected_accs = [d.expected_accuracy for d in routing_decisions]\n",
    "strategies = [d.strategy.value for d in routing_decisions]\n",
    "\n",
    "strategy_colors = {\"intrinsic\": \"blue\", \"external\": \"green\", \"human\": \"red\", \"hybrid\": \"orange\"}\n",
    "colors = [strategy_colors.get(s, \"gray\") for s in strategies]\n",
    "\n",
    "axes[0, 1].scatter(failure_probs, expected_accs, c=colors, alpha=0.7, s=100)\n",
    "axes[0, 1].set_xlabel('Failure Probability')\n",
    "axes[0, 1].set_ylabel('Expected Accuracy')\n",
    "axes[0, 1].set_title('Failure Probability vs Expected Accuracy')\n",
    "\n",
    "# Cost vs accuracy trade-off\n",
    "costs = [d.cost_estimate for d in routing_decisions]\n",
    "axes[1, 0].scatter(costs, expected_accs, c=colors, alpha=0.7, s=100)\n",
    "axes[1, 0].set_xlabel('Cost Estimate')\n",
    "axes[1, 0].set_ylabel('Expected Accuracy')\n",
    "axes[1, 0].set_title('Cost vs Accuracy Trade-off')\n",
    "\n",
    "# Strategy performance comparison\n",
    "strategy_metrics = {}\n",
    "for decision in routing_decisions:\n",
    "    strategy = decision.strategy.value\n",
    "    if strategy not in strategy_metrics:\n",
    "        strategy_metrics[strategy] = {\"accuracies\": [], \"costs\": []}\n",
    "    strategy_metrics[strategy][\"accuracies\"].append(decision.expected_accuracy)\n",
    "    strategy_metrics[strategy][\"costs\"].append(decision.cost_estimate)\n",
    "\n",
    "avg_accuracies = [np.mean(strategy_metrics[s][\"accuracies\"]) for s in strategy_counts.keys()]\n",
    "avg_costs = [np.mean(strategy_metrics[s][\"costs\"]) for s in strategy_counts.keys()]\n",
    "\n",
    "bars = axes[1, 1].bar(strategy_counts.keys(), avg_accuracies, alpha=0.7)\n",
    "axes[1, 1].set_title('Average Expected Accuracy by Strategy')\n",
    "axes[1, 1].set_ylabel('Expected Accuracy')\n",
    "\n",
    "# Add cost information as text\n",
    "for i, (bar, cost) in enumerate(zip(bars, avg_costs)):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'Cost: {cost:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost Model Analysis\n",
    "\n",
    "Let's examine how the cost model works and how different factors affect routing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cost model\n",
    "cost_summary = cost_model.get_cost_summary()\n",
    "\n",
    "print(\"Cost Model Configuration:\")\n",
    "print(\"========================\")\n",
    "print(\"\\nCost Weights:\")\n",
    "for cost_type, weight in cost_summary[\"cost_weights\"].items():\n",
    "    print(f\"  {cost_type.capitalize()}: {weight:.3f}\")\n",
    "\n",
    "print(\"\\nStrategy Cost Profiles:\")\n",
    "profiles_df = pd.DataFrame(cost_summary[\"strategy_profiles\"]).T\n",
    "display(profiles_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost profiles\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "cost_components = [\"computational\", \"monetary\", \"latency\", \"quality_risk\"]\n",
    "component_titles = [\"Computational Cost\", \"Monetary Cost\", \"Latency (seconds)\", \"Quality Risk\"]\n",
    "\n",
    "for i, (component, title) in enumerate(zip(cost_components, component_titles)):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    strategies = list(profiles_df.index)\n",
    "    values = profiles_df[component].values\n",
    "    \n",
    "    bars = ax.bar(strategies, values, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Cost')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Context Sensitivity Analysis\n",
    "\n",
    "Let's see how different contexts affect routing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different contexts\n",
    "test_prompt = \"What is the derivative of x^2 + 3x + 2?\"\n",
    "test_response = \"The derivative is 2x + 3.\"\n",
    "test_critique = \"Let me double-check this calculation...\"\n",
    "\n",
    "contexts = [\n",
    "    {\"name\": \"Low Stakes\", \"urgency\": 0.1, \"stakes\": 0.1, \"domain\": \"math\"},\n",
    "    {\"name\": \"High Stakes\", \"urgency\": 0.1, \"stakes\": 0.9, \"domain\": \"medical\"},\n",
    "    {\"name\": \"High Urgency\", \"urgency\": 0.9, \"stakes\": 0.5, \"domain\": \"general\"},\n",
    "    {\"name\": \"Complex Domain\", \"urgency\": 0.3, \"stakes\": 0.7, \"domain_complexity\": 0.9},\n",
    "    {\"name\": \"Budget Constrained\", \"urgency\": 0.5, \"stakes\": 0.6, \"budget_constraint\": 0.9}\n",
    "]\n",
    "\n",
    "context_results = []\n",
    "\n",
    "for ctx in contexts:\n",
    "    name = ctx.pop(\"name\")\n",
    "    decision = router.route(test_prompt, test_response, test_critique, ctx)\n",
    "    \n",
    "    context_results.append({\n",
    "        \"Context\": name,\n",
    "        \"Strategy\": decision.strategy.value,\n",
    "        \"Failure Prob\": decision.failure_probability,\n",
    "        \"Expected Acc\": decision.expected_accuracy,\n",
    "        \"Cost\": decision.cost_estimate\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "context_df = pd.DataFrame(context_results)\n",
    "display(context_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize context effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Strategy distribution by context\n",
    "strategy_by_context = context_df.groupby(['Context', 'Strategy']).size().unstack(fill_value=0)\n",
    "strategy_by_context.plot(kind='bar', stacked=True, ax=axes[0], alpha=0.7)\n",
    "axes[0].set_title('Strategy Selection by Context')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(title='Strategy')\n",
    "\n",
    "# Expected accuracy by context\n",
    "bars1 = axes[1].bar(context_df['Context'], context_df['Expected Acc'], alpha=0.7, color='green')\n",
    "axes[1].set_title('Expected Accuracy by Context')\n",
    "axes[1].set_ylabel('Expected Accuracy')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cost by context\n",
    "bars2 = axes[2].bar(context_df['Context'], context_df['Cost'], alpha=0.7, color='red')\n",
    "axes[2].set_title('Cost by Context')\n",
    "axes[2].set_ylabel('Cost Estimate')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1 if bars == bars1 else 2].text(\n",
    "            bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.3f}', ha='center', va='bottom', fontsize=8\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Routing Demo\n",
    "\n",
    "Try your own correction traces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive demo function\n",
    "def demo_routing(prompt, initial_response, critique, domain=\"general\", urgency=0.5, stakes=0.5):\n",
    "    \"\"\"Demo routing for custom inputs.\"\"\"\n",
    "    context = {\n",
    "        \"domain\": domain,\n",
    "        \"urgency\": urgency,\n",
    "        \"stakes\": stakes,\n",
    "        \"accuracy_requirement\": 0.8\n",
    "    }\n",
    "    \n",
    "    decision = router.route(prompt, initial_response, critique, context)\n",
    "    \n",
    "    print(f\"ROUTING DECISION\")\n",
    "    print(f\"================\")\n",
    "    print(f\"Strategy: {decision.strategy.value.upper()}\")\n",
    "    print(f\"Confidence: {decision.confidence:.3f}\")\n",
    "    print(f\"Failure Probability: {decision.failure_probability:.3f}\")\n",
    "    print(f\"Expected Accuracy: {decision.expected_accuracy:.3f}\")\n",
    "    print(f\"Cost Estimate: {decision.cost_estimate:.3f}\")\n",
    "    print(f\"\\nReasoning: {decision.reasoning}\")\n",
    "    \n",
    "    return decision\n",
    "\n",
    "# Example usage\n",
    "print(\"Example 1: Simple Math Question\")\n",
    "demo_routing(\n",
    "    prompt=\"What is 15 * 24?\",\n",
    "    initial_response=\"15 * 24 = 350\",\n",
    "    critique=\"Let me recalculate: 15 * 24 = 15 * 20 + 15 * 4 = 300 + 60 = 360\",\n",
    "    domain=\"math\",\n",
    "    urgency=0.3,\n",
    "    stakes=0.4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Example 2: Medical Question (High Stakes)\")\n",
    "demo_routing(\n",
    "    prompt=\"What are the symptoms of appendicitis?\",\n",
    "    initial_response=\"Appendicitis symptoms include stomach pain and fever.\",\n",
    "    critique=\"I should be more specific about the location and progression of pain, and mention other symptoms like nausea and loss of appetite.\",\n",
    "    domain=\"medical\",\n",
    "    urgency=0.8,\n",
    "    stakes=0.95\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Insights\n",
    "\n",
    "Let's summarize what we've learned from this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "stats = router.get_routing_statistics(routing_decisions)\n",
    "\n",
    "print(\"SCFP Framework Demo Summary\")\n",
    "print(\"===========================\")\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  ‚Ä¢ Total traces generated: {len(traces)}\")\n",
    "print(f\"  ‚Ä¢ Success rate: {sum(1 for t in traces if t.is_success)/len(traces)*100:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Failure modes: {len(set(t.failure_mode.value for t in traces))}\")\n",
    "print(f\"  ‚Ä¢ Domains covered: {len(set(t.metadata.get('domain', 'unknown') for t in traces if t.metadata))}\")\n",
    "\n",
    "print(f\"\\nüéØ Baseline Performance:\")\n",
    "best_baseline = max(results.keys(), key=lambda k: results[k]['Macro F1'])\n",
    "print(f\"  ‚Ä¢ Best baseline: {best_baseline}\")\n",
    "print(f\"  ‚Ä¢ Best F1 score: {results[best_baseline]['Macro F1']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Best accuracy: {results[best_baseline]['Accuracy']:.3f}\")\n",
    "\n",
    "print(f\"\\nüîÄ Routing Statistics:\")\n",
    "print(f\"  ‚Ä¢ Decisions analyzed: {stats['total_decisions']}\")\n",
    "print(f\"  ‚Ä¢ Average failure probability: {stats['avg_failure_probability']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Average expected accuracy: {stats['avg_expected_accuracy']:.3f}\")\n",
    "print(f\"  ‚Ä¢ Average cost: {stats['avg_cost']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìà Strategy Distribution:\")\n",
    "for strategy, info in stats['strategy_distribution'].items():\n",
    "    print(f\"  ‚Ä¢ {strategy.capitalize()}: {info['count']} ({info['percentage']:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(f\"  ‚Ä¢ The SCFP framework successfully predicts correction failures\")\n",
    "print(f\"  ‚Ä¢ Dynamic routing adapts to context (stakes, urgency, domain)\")\n",
    "print(f\"  ‚Ä¢ Cost-benefit analysis enables intelligent strategy selection\")\n",
    "print(f\"  ‚Ä¢ Different failure modes require different intervention strategies\")\n",
    "print(f\"  ‚Ä¢ The system balances accuracy, cost, and latency trade-offs\")\n",
    "\n",
    "print(f\"\\nüî¨ Next Steps:\")\n",
    "print(f\"  ‚Ä¢ Train on real correction data for better predictions\")\n",
    "print(f\"  ‚Ä¢ Implement actual external tools and human-in-the-loop systems\")\n",
    "print(f\"  ‚Ä¢ Conduct online A/B tests to validate routing decisions\")\n",
    "print(f\"  ‚Ä¢ Extend to domain-specific applications (medical, legal, etc.)\")\n",
    "print(f\"  ‚Ä¢ Develop adaptive learning from deployment feedback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This demo has shown how the SCFP framework can:\n",
    "\n",
    "1. **Predict Failures**: Identify when self-correction is likely to fail\n",
    "2. **Route Intelligently**: Select appropriate correction strategies based on context\n",
    "3. **Optimize Trade-offs**: Balance accuracy, cost, and latency requirements\n",
    "4. **Adapt to Context**: Consider domain, urgency, and stakes in decision-making\n",
    "\n",
    "The framework transforms a critical vulnerability (correction failures) into a valuable operational signal, enabling more reliable and efficient AI systems.\n",
    "\n",
    "---\n",
    "\n",
    "**To explore further:**\n",
    "- Run the full reproduction script: `./scripts/reproduce_all.sh`\n",
    "- Try the interactive routing demo: `python scripts/demo_routing.py --interactive`\n",
    "- Examine the complete implementation in the `src/` directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
